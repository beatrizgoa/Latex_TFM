\section{Metrics}
In this section, the metrics that have used to express the results obtained, in each experiment, are going to be explained.

\subsection{Cost and Error}
The first parameter that is used is the cost. The cost is used while the neural network is training, in fact, is the value that must be minimized during the training. The lower value, The better performance of the network.\\

The cost calculated with the Minibatch Stochastic Gradient Descent (MSGD) is the Negative Log-Likelihood Loss. The MSGD is a variant of the Stochastic Gradient Descent in which the cost is calculated with a mini batch of data, not each sample independently and the Loss is the accumulation \cite{Stutz}.\\

The loss is calculated in the following way:\\

\begin{equation}
  Loss(\theta, D) = - \sum_{i=0}^{|D|}log P(Y = y^{(i)}|x^{(i)}, \theta)
\end{equation}


\subsection{True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN)}
If each predicted class is compared with it real target, True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN) values could be calculated. These metrics are used for bi-classes problems, when there is a positive class and a negative class. The positive class, in the particular example of this thesis, is being real users, genuine user or \textit{bona fide} and the negative class is the attack class.\\

Those metrics, are gotten when a positive or negative samples is well or misclassified \cite{Sokolova}. The classified sample or predicted is compared with it real target.\\

If a positive sample is classified as positive is a true positive (TP), but if it has been classified as negative is a false negative (FN).\\
If a negative sample is classified as negative is a true negative (TN), but if it has been classified as positive, is a false positive (FP).\\

From those four metrics, it could be extracted the confusion matrix for binary classification which is defined in \ref{table:ConfusionMatrix} \cite{ROC}:

\begin{table}[]
\centering
\begin{tabular}{|
>{\columncolor[HTML]{EFEFEF}}c |>{\columncolor[HTML]{FFFFFF}}c | >{\columncolor[HTML]{FFFFFF}}c |}
\hline
\textbf{Real / Classified} & \cellcolor[HTML]{EFEFEF}\textbf{Positive} & \cellcolor[HTML]{EFEFEF}\textbf{Negative} \\ \hline
\textbf{Positve}           & TP                                        & FN                                        \\ \hline
\textbf{Negative}          & FN                                        & TN                                        \\ \hline
\end{tabular}
\caption{Confusion Matrix} \label{table:ConfusionMatrix}
\end{table}

The confusion Matrix resume those four metrics in a simple table. Both the confusion matrix or the parameters individually are widely utilized.\\

\subsection{ROC curve and Precision and Recall curve}
From the confusion matrix, it is possible calculate others parameters \cite{Sokolova}: precision, recall, specificity, accuracy because its values depend on TP, TN, FP, FN:\\
\begin{itemize}
\item The False Positive Rate (FPR) is defined as the proportion of all the negative samples (\textit{N}) that are classified as positive incorrectly \cite{ROC}:

\begin{equation}
FPR = FP / N
\end{equation}

\item The True Positive Rate (TPR) is defined as the proportion of all the positive samples (\textit{P} that are classified correctly \cite{ROC}: 

\begin{equation}
FPR = TP / P
\end{equation}

%TPR could be called sensitivity and FPR could be calculated as 1 â€“ specificity \\

\item Precision

\begin{equation}
  precision = \frac{TP}{TP + FP}
\end{equation}
\item Recall

\begin{equation}
  recall = \frac{TP}{TP + FN}
\end{equation}
\item Accuracy

\begin{equation}
  Accuracy = \frac{TP + TN}{TP + FP + FN + TN}
\end{equation}

\end{itemize}

 as could be seen in it respective equation:\\








The Receiver Operator Characteristic (ROC) curves is the representation how the number of positives samples which has been classified correctly changes with the number of negative samples incorrectly classified. The ROC curve is defined by the parameters False Positive Rate (FPR) and True Positive Rate (TPR) \cite{ROC}.\\

The Precision and Recall curve is the representation of the Precision and the Recall in the same graph.\\

\subsection{APCR and BPCR}
The ISO/IEC 30107-3 \cite{ISO}

Attack Presentation Classification Error Rate (APCER) is defined as the proportion of presentation attacks that has been classified incorrectly (as \textit{bona fide} presentation.)\\

\begin{equation}
  APCER_{PAIS} = \frac{1}{N_{PAIS}}\sum_{i=1}^{N_{PAIS}}(1 - Res_{i})
\end{equation}

\textit{Bona fide} Presentation Classification Error Rate (BPCER) is defined as the proportion of \textit{bona fide} presentations  incorrectly classified as presentation attacks.\\

\begin{equation}
  BPCER = \frac{\sum_{i=1}^{N_{BF}}Res_{i}}{N_{BF}}
\end{equation}


where: \begin{itemize}
\item $N_{BF}$ is the number of \textit{bona fide} presentations
\item $Res_{i}$ is 1 if $i^{th}$ presentation is classified as an attack and 0 if is classified as a \textit{bona fie} presentation.
\item $N_{PAIS}$ is the number of attack presentations
\end{itemize}
