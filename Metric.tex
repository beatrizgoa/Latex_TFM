\section{Metrics}
In this section, the metrics that have used to express the results obtained, in each experiment, are going to be explained.

\subsection{Cost and Error}
The first parameter that is used is the cost. The cost is used while the neural network is training, in fact, is the value that must be minimized during the training. The lower value, The better performance of the network.\\

The cost calculated with the Minibatch Stochastic Gradient Descent (MSGD) is the Negative Log-Likelihood Loss. The MSGD is a variant of the Stochastic Gradient Descent in which the cost is calculated with a mini batch of data, not each sample independently and the Loss is the accumulation \cite{Stutz}.\\

The loss is calculated in the following way:\\

\begin{equation}
  Loss(\theta, D) = - \sum_{i=0}^{|D|}log P(Y = y^{(i)}|x^{(i)}, \theta)
\end{equation}


\subsection{True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN)}
If each predicted class is compared with it real target True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN) values could be calculated. These metrics are used for bi-classes problems, when there is a positive class and a negative class. The positive class, in the particular example of this thesis is being real users, genuine user and \textit{bona fide} and the negative class is the attack class.\\

Those metrics, are gotten when a positive or negative samples is well or misclassified \cite{Sokolova}.\\

If a positive sample is classified as positive is a true positive (TP), but if it has been classified as negative is a false negative (FN).\\
If a negative sample is classified as negative is a true negative (TN), but if it has been classified as positive, is a false positive (FP).\\

From those four metrics, it could be extracted the confusion matrix for binary classification which is defined in \ref{table:ConfusionMatrix}:

\begin{table}[]
\centering
\begin{tabular}{|
>{\columncolor[HTML]{EFEFEF}}c |>{\columncolor[HTML]{FFFFFF}}c | >{\columncolor[HTML]{FFFFFF}}c |}
\hline
\textbf{Real / Classified} & \cellcolor[HTML]{EFEFEF}\textbf{Positive} & \cellcolor[HTML]{EFEFEF}\textbf{Negative} \\ \hline
\textbf{Positve}           & TP                                        & FN                                        \\ \hline
\textbf{Negative}          & FN                                        & TN                                        \\ \hline
\end{tabular}
\caption{Confusion Matrix} \label{table:ConfusionMatrix}
\end{table}

The confusion Matrix resume those four metrics in a simple table. Both the confusion matrix or the parameters individually are widely utilized.\\

\subsection{ROC curve}
From the confusion matrix, it is possible calculate others parameters \cite{Sokolova}: precision, recall, specificity, accuracy because its values depend on TP, TN, FP, FN as could be seen in it respective equation:\\
\begin{equation}
  precision = \frac{TP}{TP + FP}
\end{equation}

\begin{equation}
  recall = \frac{TP}{TP + FN}
\end{equation}

\begin{equation}
  Accuracy = \frac{TP + TN}{TP + FP + FN + TN}
\end{equation}

The ROC curve is the representation of \\

\subsection{Precision and Recall}
The Precision and Recall curve is the representation of the Precision and the Recall in the same graph.\\

\subsection{APCR and BPCR}
The ISO/IEC 30107-3 \cite{ISO}

Attack Presentation Classification Error Rate (APCER) is defined as the proportion of presentation attacks that has been classified incorrectly (as \textit{bona fide} presentation.)\\

\begin{equation}
  APCER_{PAIS} = \frac{1}{N_{PAIS}}\sum_{i=1}^{N_{PAIS}}(1 - Res_{i})
\end{equation}

\textit{Bona fide} Presentation Classification Error Rate (BPCER) is defined as the proportion of \textit{bona fide} presentations  incorrectly classified as presentation attacks.\\

\begin{equation}
  BPCER = \frac{\sum_{i=1}^{N_{BF}}Res_{i}}{N_{BF}}
\end{equation}


where: \begin{itemize}
\item $N_{BF}$ is the number of \textit{bona fide} presentations
\item $Res_{i}$ is 1 if $i^{th}$ presentation is classified as an attack and 0 if is classified as a \textit{bona fie} presentation.
\item $N_{PAIS}$ is the number of attack presentations
\end{itemize}
