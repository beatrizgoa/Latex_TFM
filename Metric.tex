\section{Metrics}
In this section, the metrics that have used to express the results are going to be shown and explained.

\subsection{Cost and Error}
The first parameter that is used is the cost. The cost is used while the neural network is training. The lower value, The better performance of the network.\\

More specifically, for training the neural network. \\

\subsection{True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN)}
True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN) are metrics that are used for bi-classes problems, positive class and negative class. The positive class is real users, genuine user and \textit{bona fide} and negative class is the attack class.\\

Those metrics, are gotten when a positive or negative samples is well or misclassified \cite{Sokolova}.\\

If a positive sample is classified as positive is a true positive (TP), but if it has been classified as negative is a false negative (FN).\\
If a negative sample is classified as negative is a true negative (TN), but if it has been classified as positive, is a false positive (FP).\\

From those four metrics, it could be extracted the confusion matrix for binary classification, explained in \ref{table:ConfusionMatrix}:

\begin{table}[]
\centering
\begin{tabular}{|
>{\columncolor[HTML]{EFEFEF}}c |>{\columncolor[HTML]{FFFFFF}}c | >{\columncolor[HTML]{FFFFFF}}c |}
\hline
\textbf{Real / Classified} & \cellcolor[HTML]{EFEFEF}\textbf{Positive} & \cellcolor[HTML]{EFEFEF}\textbf{Negative} \\ \hline
\textbf{Positve}           & TP                                        & FN                                        \\ \hline
\textbf{Negative}          & FN                                        & TN                                        \\ \hline
\end{tabular}
\caption{Confusion Matrix} \label{table:ConfusionMatrix}
\end{table}

\subsection{ROC curve}
From the confusion matrix, it is possible calculate others parameters \cite{Sokolova}: precision, recall, specificity, accuracy because its values depend on TP, TN, FP, FN as could be seen in it respective equation:\\
\begin{equation}
  precision = \frac{TP}{TP + FP}
\end{equation}

\begin{equation}
  recall = \frac{TP}{TP + FN}
\end{equation}

\begin{equation}
  Accuracy = \frac{TP + TN}{TP + FP + FN + TN}
\end{equation}

The ROC curve is \\

\section{Precision and Recall}

\subsection{APCR and BPCR}
The ISO/IEC 30107-3 \cite{ISO}

Attack Presentation Classification Error Rate (APCER) is defined as the proportion of presentation attacks that has been classified incorrectly (as \textit{bona fide} presentation.)\\

\begin{equation}
  APCER_{PAIS} = \frac{1}{N_{PAIS}}\sum_{i=1}^{N_{PAIS}}(1 - Res_{i})
\end{equation}

\textit{Bona fide} Presentation Classification Error Rate (BPCER) is defined as the proportion of \textit{bona fide} presentations  incorrectly classified as presentation attacks.\\

\begin{equation}
  BPCER = \frac{\sum_{i=1}^{N_{BF}}Res_{i}}{N_{BF}}
\end{equation}


where: \begin{itemize}
\item $N_{BF}$ is the number of \textit{bona fide} presentations
\item $Res_{i}$ is 1 if $i^{th}$ presentation is classified as an attack and 0 if is classified as a \textit{bona fie} presentation.
\item $N_{PAIS}$ is the number of attack presentations
\end{itemize}
