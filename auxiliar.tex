\section{Start working with Faces databases}
The project has been developed from LeNet, changes would be explained below.\\

\subsection{Using Labeled Faces in the Wild}
In order to read and work with face images, a own script has been developed where images are read, re-sized if it is wanted or required. All images are pseudo-randomized dived into train(49\% of the total database), test(30\% of the total data) and validation sets (21\% of the data).\\

To split the data, train\_test\_split function from sklearn.cross\_validation has been used. This function pseudo-randomize the data, so you can repeat the randomized split data in the same way assigning the same seed to the function.\\

If the batch size is 500, 13 batches are going to be used for training, 6 for validation and 6 for testing too.\\

\begin{itemize}
\item $learning_rate=0.1, n_epochs=12, nkerns=[20, 50], batch_size=500$
\item  T.tanh activaction function
\item rng = numpy.random.RandomState(23455)
\item kernel size = [5,5]
\item images resized to 28x28
\item 12 epoch
\item The number of neurons at the input of the regression class is 10000, and the number of neurons at the output is 5748 (Number of people).
\end{itemize}

The results obtained are not good, because of the fact that parameters have been chosen randomly. \\

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{images/epoch_LFW.png}
\caption{Error of Lenet using LFW.} \label{fig:LENETLFW}
\end{figure}

Figure \ref{fig:LENETLFW} represents the validation error \% in each epoch, and it could be seen that in the last epoch, the error is the smallest one, and the test error in that point is 95.125000 \%.\\

In order to know how the net works with different learning rates, it has been changed to 0.001 and the number of epoch has been raised to 50.\\

The error in each epoch could be seen in figure \ref{fig:LENETLFWerror0-001} , where it is shown that the net does not learn because the learning rate is too small and it would need more epoch. The cost function of the training could be seen in figure \ref{fig:LENETLFWcost0-001}, where it has been reducing during epochs, but it has been reduced a bit, from 8.7 to 8.1.\\

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{images/LFW_learningrate/error_0_001.png}
\caption{Error of Lenet using LFW with a learning rate of 0.001.} \label{fig:LENETLFWerror0-001}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{images/LFW_learningrate/cost_0_001.png}
\caption{Cost of Lenet using LFW with a learning rate of 0.001.} \label{fig:LENETLFWcost0-001}
\end{figure}

A 97,73\% error of test performance has been gotten, which has been obtained in iteration 13. \\

The conclusion is the learning rate is too small to this configuration of the net and the data given.\\

\subsubsection{Changing learning rate}
Despite the fact that the configuration of the networks is not to this database, learning rate is going to be changed so it is possible to know how it affects.\\

If the learning rate is increased to 0.01: Best validation score of 96.266667 \% obtained at iteration 481, with test performance 95.733333 \%\\

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{images/LFW_learningrate/error_0_01.png}
\caption{Error of Lenet using LFW changing learning rate to 0.01.} \label{fig:LENETLFW_lr0_01}
\end{figure}

If learning rate is increased to 0,1:Best validation score of 96.300000 \% obtained at iteration 13, with test performance 95.733333 \%\\

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{images/LFW_learningrate/error_0_1.png}
\caption{Error of Lenet using LFW changing learning rate to 0.1.} \label{fig:LENETLFW_lr0_1}
\end{figure}

If learning rate = 0.5 Best validation score of 96.3 \% obtained at iteration 143, with test performance 95.73\% \\

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{images/LFW_learningrate/error_0_5.png}
\caption{Error of Lenet using LFW changing learning rate to 0.5.} \label{fig:LENETLFW_lr0_5}
\end{figure}

In conclusion, if the learning rate is too big, the network do not get a optimal minimum and if the learning rate is too small it takes too mch iteration to learn or getting to to optimal minimum.\\


\subsubsection{Changing convolutional parameters}
Convolutional layers are built with the Theano function \textit{theano.tensor.nnet.conv2d}. The size of convolutional layers depends on the number of filters that users would like, the dimension of images, it is not possible to use the same convolutional layer for 3d images (rgb) than grey scale images (1 dimension), and also depends on the high and width user would like to give.\\

The output of a convolutional layer depends on the size of the filter and the size of input images. At the output, a new bunch of images are created from the input images and the characteristics of the layer.\\

Also, it is important to consider the batch size, because the layer is not fed by a individual image; the layer is fed by the bunch of images.\\

Lets have a bit of fun with convolutional layers, the number of filters and the size of them it is going to be changed. A learning rate of 0.1 is going to be used for 50 epoch.\\

In the first example, the number of filters of the first layer is going to be increased from 20 to 40, and the number of the second layer from 40 to 60.

In figure \ref{fig:LENETLFW_ker1} could be seen the error which has been gotten in each epoch, where the best vest validation score of 94.9\% has been obtained at iteration 559, with test performance 95\%.

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{images/LFW_layers/error_conv_40_60.png}
\caption{Error of Lenet using LFW changing number of kernels in example 1.} \label{fig:LENETLFW_ker1}
\end{figure}

If the number of filters has been increased, the time that the code takes to run is increased significantly. Also, the results of the network changes, this is a parameter that should have in consideration.\\

Also, it is possible to modify the filter shape, in the previous examples, the size of both kernels were 5x5, in this example, the second one, the first convolutional layer would have a filter size of 3x3, the second one would keep its original size.\\

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{images/LFW_layers/error_conv_40_60_2.png}
\caption{Error of Lenet using LFW changing number of kernels and the filter size in example 2.} \label{fig:LENETLFW_ker2}
\end{figure}

It is interesting seeing how the error of the network has been improved when the number of kernels has been changed and, in this example, has been improved too when the size of the filter has been changed.\\

The error in each epoch of this example could be seen in figure \ref{fig:LENETLFW_ker2}, where the best validation score obtained has been 94.567\% error iteration 611, with test performance error 93.967\%.\\

In order to see the difference between using a big size filter and one of a small size, in the next example, third example, 40 and 60 kernels are going to be used, and the size of each one is 3 and 10 for layer 0 and layer 1 respectively. In epoch number 40, the weighs of each layer has been saved and in figure X are represented. At irst sight, it is possible to see that with a big size.\\

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{images/LFW_layers/error_conv_40_60_3.png}
\caption{Error of Lenet using LFW changing number of kernels and the filter size in example 3.} \label{fig:LENETLFW_ker3}
\end{figure}

Also, it is possible to see the output at the convolutional layer, in figure \ref{fig:LENETLFW_ker4}, the first 25 output images are shown for both layers.\\

\begin{figure}[htb]
\centering
\includegraphics[width=1.4\textwidth]{images/LFW_layers/conv_0_out.png}
\includegraphics[width=1.4\textwidth]{images/LFW_layers/conv_1_out.png}
\caption{Output of the convolutional layers using LeNet and LFW.} \label{fig:LENETLFW_ker4}
\end{figure}

The result of the third example is a best validation score of 95.73 \% obtained at iteration 546, with test performance 95.23 \%. \\


The cost function og those three experiments are represented in \ref{fig:LENETLFW_Cost}

\begin{figure}[htb]
    \centering
        \subfigure[Example 1]{\includegraphics[width=0.47\textwidth]{images/LFW_layers/cost_conv_40_60.png}}
        \subfigure[Example 2]{\includegraphics[width=0.47\textwidth]{images/LFW_layers/cost_conv_40_60_2.png}}
		\subfigure[Example 3]{\includegraphics[width=0.47\textwidth]{images/LFW_layers/cost_conv_40_60_3.png}}
    
    \caption{Cost function at training of the three examples chanching the convolutional parameters.} \label{fig:LENETLFW_Cost}
\end{figure}

%\subsubsection{Changing pooling parameters}
%The theano function used to build this kind of layer depends of theano version, 


\subsection{Using ReLu as an activation function instead of tanh}
Originally LeNet uses as activation function tanh(), but in this section, ReLu (Rectified linear units) activation function is going to be used. In equation \ref{Relu-formula} is shown how it is defined.  \\

\begin{equation}
f(x) = max(0,x)
\label{Relu-formula}
\end{equation}

The error and the cost in each epoch could be seen in figure \ref{fig:LENET_relu} 

\begin{figure}[htb]
\centering
		\subfigure[Cost at training]{\includegraphics[width=0.47\textwidth]{images/images_lenet/cost_relu.png}}
		\subfigure[Error at validation]{\includegraphics[width=\textwidth]{images/images_lenet/error_relu.png}}

\caption{Error and cost using ReLu instead of tanh}  
\label{fig:LENET_relu}
\end{figure}



\subsection{using FRAV dataset}
The experiments made with this database are just with RGB images (for the time being), so 939 images of people has been  used, 489 train images has been used, 162 validation images and 279 test images.\\
 
Because images has not the same shape, they have been re-sized into 252x180, this new shape is proportional 0.7*height = weight because all images studied save that proportion. In addition, making images smaller also gives the security of not having memory problems, because the huge quantity of used images.\\ 
 
The network has been tested with this databases in the two ways of classify images, with two classes (genuine and attacks) and five classes (genuine and four classes, one per type of attack). Also, different learning rates have been used.\\


The first experiment made was based in the neural network LeNet, without changing parameters:\\

\begin{itemize}
\item 25 epoch
\item nkerns=[20, 50]
\item batch\_size=50
\item learning rate = 0.01
\item Logistic regresion with 10 neurons.\\
\end{itemize}

The results that were obtained there were not as bad as the one with labeled faces in the wild (LFW).\\
 
Figure \ref{fig:FRAV_five} shows the validation error of five classes, the best validation error has token place in epoch 7 with test performance 60.4\%. It has token 68.78 minutes to run.\\

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{images/epoch_5classes_FRAV_1.png}
\caption{Error using FRAV database and five classes.} 
\label{fig:FRAV_five}
\end{figure}

In figure \ref{fig:FRAV_two}, the validation error in different epoch could be visualized using two classes to classify. It could be seen that in each epoch the validation error is the same. Test error performance at the first epoch is 23.2 \%. The total time of the running has been 64.0167 minutes.\\

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{images/epoch_2classes_FRAV_1.png}
\caption{Error using FRAV database and two classes.}  
\label{fig:FRAV_two}
\end{figure}

After this, the CNN has been changed in order to build a CNN similar to the one described in \textit{Learning Temporal Features Using LSTM-CNN Architecture for FaceAnti-spoofing}.\\

This CNN has two conv. nets., the first one with 48 filters and the second one with 96. The size of the filters is 5x5. The conv. nets. are followed by max pool layers of size 2x2.\\

The activation function is the rectified linear activation function (ReLu). I have used a normalized distribution of weights and bias, the same which was implemented in LeNet: weights are sampled randomly from a uniform distribution in the range [-1/fan-in, 1/fan-in], where fan-in is the number of inputs to a hidden unit. To use ReLu I have used the one implemented in \textit{theano.tensor.nnet.relu}.\\

Because of the huge number of images, the  batch\_size has been changed into 20. And the learning rate is 0.001, like in the paper.\\
 
The number of neurons at the output of the hidden layer are 100. The classifier is the sigmoidal function. 25 epoch have been used to run the net at the training.\\
 
This database has been run with different seeds when the data is split in order to observe the behavior of the net with different sets of data.\\

%(AÑADIR LOS RESULTADOS CON LAS DIFERENTES SEMILLAS)

%\subsection{using CASIA dataset}
%Before proving casia databse in the convolutional neural network, the net has been modified in order to diference training and test, so the net train and validate and when a new best score of error is gotten, the network is saved. After training, the best configuration of the network is loaded and tested with the test set.\\

%From the database, images have been resized in order to save memory, into 52x104 images. Also, two different ways of using classes has been used, first, two classes has been used, one the imges of the genuine people and another class with the atacks, the last way of separate classes is assigning one different class to a different attack.\\

%Casia database has also been proved using two classes and two seeds.\\

%(AÑADIR LOS RESULTADOS)


\section{Regenerating the databases}
The databases have been generated in a different way from the used before. In this new way, the possibility of access to the missclasified samples is possible. SO the characteristic of misclassified images could be studied. This is not used now, but in the future it could be useful.\\

In order to build the new database, a script as been programmed manually because the test-train split sklearn function does not let know what image has been read and now it is possible to compare the right class of each image with the predicted one.\\

All this work has been changed because it is interesting to know the characteristics of people (if has beard, glasses or long hair) that are not well classified.\\

It has been necessary to shuffle the data twice in order to mix it properly and traning, testing and validating test would have data of the five classes.\\

\subsection{using RGB and NIR FRAV database}
To use both images at the same time, it is possible to do it in two different ways:
\begin{itemize}
\item Characteristic level: adding the NIR image as another layer to RGB image, so the resultant image have hightxweightx4 dimensions (NIR images has one layer because it is a grey scale image and RGB images has tree layers, one per each primary color). The network is feed with the resultant images like other times.
\item Classification level: training two network twice, first, with RGB images and then with NIR images and when the classification is produced combine them.
\end{itemize}


\subsubsection{Adding images in characteristic level}
In order to get this particular goal, each image is re-sized proportionally to original image to 52x104 dimensions. Each NIR image is appended to its correspondent RGB image.\\

In order to create a list of images where images are not putted in order, each image followed by another could be of a different class. Two shuffles has been necessary, the first one with a seed = 0.5 and the second one with a seed = 0.1, so the randomize order of images could be repeated.\\ 

For training, the 70\% of the total data has been used, for testing the 20\% and for validating the resting 10\%. There are 157 images in each class and the distribution is represented in the table \ref{FRAV_distribution1}.\\

\begin{table}[]
\centering
\label{FRAV_distribution1}
\begin{tabular}{c|ccccc|c|}
\cline{2-7}
                                     & \multicolumn{1}{c|}{Class 0} & \multicolumn{1}{c|}{Class 1} & \multicolumn{1}{c|}{Class 2} & \multicolumn{1}{c|}{Class 3} & Class 4 & Total of samples \\ \hline
\multicolumn{1}{|c|}{Training set}  & 94                           & 119                          & 116                          & 145                          & 76      & 550              \\ \cline{1-1}
\multicolumn{1}{|c|}{Testing set}    & 33                           & 38                           & 37                           & 7                            & 42      & 157              \\ \cline{1-1}
\multicolumn{1}{|c|}{Validating set} & 30                           & 0                            & 4                            & 5                            & 39      & 78               \\ \hline
\end{tabular} \caption{Distribution of samples FRAV (RGB + NIR) database}

\end{table}

The code runs for 150 epoch with a learning rate of 0.001 and a batch size of 20 images.\\

The disadvantage of using mini-batches is that there are some images remain and the quantity os less than a mini-batch, that images are not used, so from 157 testing images, just 140 has been used.\\

The test error that has been gotten is 30\% at iteration 1863 where the best validation score was gotten (26,67\%). Where:

\begin{itemize}
\item Class 0 has been misclassified  14 times
\item Class 1 has been misclassified  6 times
\item Class 2 has been misclassified  7 times
\item Class 3 has been misclassified  2 times
\item Class 4 has been misclassified  2 times
\end{itemize}

To get that results 3,04 hours was needed to run the code.\\

%y_pred = [2,4,2,1,2,1,2,1,4,0,3,1,2,1,2,0,2,0,3,1,3,1,3,1,2,1,2,1,4,1,2,4,2,1,2,1,2,1,2,1,2,1,3,1,2,1,2,0,2,1,2,1,3,1,2,1,2,1,3,1,4,1,2,1,2,1,2,1,3,1,2,1,2,1,2,1,0,4,3,4,0,1,1,4,1,4,1,4,2,4,0,4,1,4,0,4,1,4,2,4,1,4,1,1,0,4,0,4,2,4,0,4,1,4,0,4,0,4,3,4,4,4,0,4,1,4,4,4,2,4,0,4,1,4,1,4,1,4,3,4]

%y_real = [2,1,2,1,2,1,2,1,4,1,2,1,2,1,2,1,2,1,3,1,2,1,2,1,2,1,2,1,4,1,2,1,2,1,2,1,2,1,3,1,2,1,2,1,2,1,2,1,2,1,2,1,2,1,2,1,2,1,3,1,2,1,2,1,2,1,2,1,2,1,2,1,2,1,2,1,0,4,3,4,0,4,0,4,0,4,0,4,2,4,0,4,0,4,0,4,0,4,3,4,0,4,0,4,0,4,0,4,2,4,0,4,0,4,0,4,0,4,3,4,0,4,0,4,0,4,0,4,2,4,0,4,0,4,0,4,0,4,3,4,0,4,0,4,0,4,0,4,2,4,0,4,0,4,0,4,0]


%\section{Metrics}
%In order to measure the net and its works, some metrics would be used, but %to do that, the data has to be splited in other way because with the %function train\_test\_split there is no chance, which is know, to get the %image if it is not printed; so a own method has been developed and it is %posible too pseudo-randomize the data.\\

%The metrics that has been used are:
%\begin{itemize}
%\item FAR: False aceptance rate
%\item FRR: False rejection rate
%\end{itemize}

\clearpage

\subsection{Architecture implemented in Casia videos}
In this section, the architecture used in the paper of the Casia videos would be repeated partially. The paper is learn convolutional neural network for face anti-spoofing.\\

The architecture followed in the paper is the same used in Imagenet, it is formed by five convolutional layers, followed by three fully-connected layers. The two first conv layer and the last one are followed by a max-pool layer. The two first conv layers are followed by response normalization layers too. Authors use ReLu like activation function in each layer. The first two fully-connected layers are followed by two dropout layers, and the last layer output is followed by softmax.\\

Authors do not explain anything else about the architecture. In the paper of Imagenet is said that:

\begin{itemize}
\item The ReLU non-linearity is applied to the output of every convolutional and fully-connected layer.
\item The first convolutional layer filters the 224×224×3 input image with 96 kernels of size 11×11×3 with a stride of 4 pixels.
\item The second convolutional layer takes as input the (response-normalized and pooled) output of the first convolutional layer and filters it with 256 kernels of size 5 × 5 × 48.
\item The third, fourth, and fifth convolutional layers are connected to one another without any intervening pooling or normalization layers.
\item The third convolutional layer has 384 kernels of size 3 × 3 × 256 connected to the (normalized, pooled) outputs of the second convolutional layer. 
\item The fourth convolutional layer has 384 kernels of size 3 × 3 × 192.
\item  The fifth convolutional layer has 256 kernels of size 3 × 3 × 192. 
\item The fully-connected layers have 4096 neurons each.
\item The maxpool size filter si 2x3 because it reduces the error 0.4\% - 0.6\% compared to 2x2 filters.
\item In Imagenet uses 224x224 images.
\end{itemize}

It is not explained how authors change the architecture of Imagenet.\\

The data used to this experiment is the Casia database, the one that uses in the paper, although authors, use Reply-attack too.\\ 

The data is composed by two folders, a training folder and a test data, in each folder there are some user folders, in each user folders there are two videos from the real user, tho videos of the user with a mask, two videos with the mask and with a hole in the eyes and two videos with a digital screen where a user is shown in the screen. 3 attacks are presented, so four classes are used (real users, users with mask, users with mask and eyes and a digital screen). It is not specified how many frames authors use, so it s supposed that they use all the video.\\

For each frame, the face is looked for in the image with viola jones algorithm implemented in opencv, and the image is cropped and saved, but in that image cropped there is no background, so different scales, 1.4, 1.8, 2.2, 2.6, are used to get background, because in learn convolutional neural network for face anti-spoofing and then images are re-sized to 128x128.\\

In order to carry out this experiment, a better computer has been needed because it was no possible to run with the same that the utilized in the previous experiments.\\

But with a better computer, it is not possible read more than 2 or 3 frames per video to run the net, because it has a huge architecture with a big quantity of filters per layer.\\

So the experiment with the videos has not been possible to be carried out.\\

In addition, is it not possible to carry out the architecture of imagenet with the size of Casia images because if it is started with 128x128 images, at the end,  images sizes are images of < 1px.\\

In order to know how strides work, a python file has been created called understanding strides, in which a pickle format file is loaded. This pickle format file is the output of a layer, and it is possible to see the size of images of the layer. So it is possible to know the size of images after striding and this number could be saved to be written in the conv layer.\\

In this computer, the theano function used to build the max\-pool layer has changed bacuse of Theano version, the previous function used in Theano  \textit{theano.te\-nsor.sig\-nal.down\-sam\-ple.max\-\_pool\_2d} has been replaced by \textit{thea\-no.ten\-sor.sig\-nal.pool.pool\_2d} using the mode \textit{max}.\\

To  sum up, In this first experiment, the architecture is formed by the convolutional and pooled out layers but without strides. The folder where this experiment has been developed has been in frav\_casia\_imagenet. The architecture is just based in conv, pool and hidden layer (no dropout, softmax or normalization layer). In figure \ref{fig:error_imagenet1} it is possible to see how the error is descending in each epoch and get stabilized with a 5\% aprox. error. \\
\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{images/imagenet/error_frav-Imagenet1.png}
\caption{Error of Lenet in the first try to build imagenet.} \label{fig:error_imagenet1}
\end{figure}

The code had been running for 364,39 hours. And the true positive rate, true negative rate, false positive rate and false negative rate are available in table \ref{tabla_error_imagenet1}. \\

\begin{table}[htb]
\centering
\label{tabla_error_imagenet1}
\begin{tabular}{lllll}
\cline{1-4}
\multicolumn{1}{|l}{TP} & \multicolumn{1}{l}{TN} & \multicolumn{1}{l}{FP} & \multicolumn{1}{l|}{FN} &  \\ \cline{1-4}
\multicolumn{1}{|l}{156} & \multicolumn{1}{l}{623} & \multicolumn{1}{l}{10} & \multicolumn{1}{l|}{11} &  \\ \cline{1-4}
                         &                         &                         &                         &  \\
                         &                         &                         &                         & 
\end{tabular}
\caption{TP, TN, FP, FN rates in the first trying of imagenet.}

\end{table}



\clearpage
%\subsection{Reading frav faces}
%He hecho un base de datos generator, en el que voy creando la base de datos, dentro de esa carpeta hay un info en el que se explica como se leen las imagenes. Puedes escalarlas, puedes buscar caras o no y en guardarlas en distintas escalas como en casia.\\


\subsection{As close as possible as Imagenet}
%Dentro de la carpeta de FRAv\_casia\_ImageNet, en imagenet2, se ha programado imagenet, lo unico que sin strides en la primera capa de convolucion.\\
Because of the lack of information about the convolutional neural network described in casia paper, it has been necessary  to though the original code that authors use, Imagenet. Authors use the same architecture, although how it has been modified it is not explained. \\

It has been possible to implement Imagenet, the difference between the implementation and the original one is that the strides used in the first convolutional layer has not been used because the input of the images need to be bigger to have more than one neuron in the last layer.\\ 
 
%Dentro hay una carpeta para cada base de datos. Dentro de cada carpeta estan las pruebas que se hacen con cada base de datos.\\

The convolutional neural network has been tested with different databases: FRAV, CASIA and MFSD and different experiments have been carried out with each one.\\

\subsubsection{Network configuration for each experiment}
The configuration of each experiment of each network are described in the following lines. When it is said that Gaussian weight initialization has been used, the mean value is 0 and the std used is 0.01 in all the times ans bias has been initialized to 1, if not, bias has been initialized to 0. When Gaussian initialization has not been used,  weights are sampled randomly from a uniform distribution in the range [-1/fan-in, 1/fan-in], where fan-in is the number of inputs to a hidden unit [copy-paste from deeplearning.net]. \\ 

The goal of the next experiments is getting an optimal architecture changing the parameters. the difference between using a normal distribution o a Gaussian distribution of weights initialization has been tested and using SVM with linear kernel and RBF kernel has been tried.\\

FRAV database (Common parameters:  n\_epochs=400, nkerns=[96, 256, 386, 384, 256], batch\_size=20):\\
\begin{itemize}
\item frav1: Gaussian weights initialization. SOFTMAX used as classifier. Learning rate = 0.001 Figure \ref{fig:Imagenet2-frav1}.
\item  frav\_gaussian\_initinizialization: Gaussian weight Initialization. Learning rate = 0.001. SOFMTAX used as classifier. Figure \ref{fig:Imagenet2-frav_gaussian_init} .
\item svm\_gauss: Using SVM (with RBF kernel) as classifier. Gaussian weight Initialization. Learning rate = 0.01 \ref{fig:Imagenet2-frav-svm_gauss}.
\item svm\_genera: Using as a classifier SVM with RBF kernel. Learning rate = 0.01 \ref{fig:Imagenet2-mfsd-svm_general}.
\item svm\_linear: Classifying with SVM (linear) and Gaussian weight initialization. Learning rate = 0.01\\
\end{itemize}

CASIA (images) ( nkerns=[96, 256, 386, 384, 256]):\\
First, four test has been carries out (in which the learning rate has been changed and the number of epochs. trying to get the best learning rate configuration. In four test the batch size used is 25 samples, the classifier used at testing is Softmax and the weight initialization used is normal distribution.:\\
\begin{itemize}
\item Test1: learning\_rate=0.01, nepochs=400.
\item Test2: learning\_rate=0.001, n\_epochs=400.
\item Test3: learning\_rate=0.0005, n\_epochs=400.
\item TEst4: learning\_rate=0.001, n\_epochs=1000,
\end{itemize}

It has not been possible getting a train loss that converge in a minimum in none of the four tests. A good train loss should decrease in each epoch until converge in a minimum. But in the tests, the 
- casia\_gaussian\_init: gausiana de pesos con mean 0 y std 0.01. SOFMTAX como clasificador. learning\_rate=0.01, n\_epochs=400, nkerns=[96, 256, 386, 384, 256], batch\_size=20 \\
- svm\_gauss: Utilizando svm (rbf) como clasificador, con inizializacion normal. learning\_rate=0.01, n\_epochs=400, nkerns=[96, 256, 386, 384, 256], batch\_size=20\\
- svm\_general: utilizando SVM (rbf) con inicializacion de pesos gaussiana. learning\_rate=0.01, n\_epochs=400, nkerns=[96, 256, 386, 384, 256], batch\_size=20\\
- svm\_linear: SVM (linear) con inicializacion de pesos gaussiana. learning\_rate=0.01, n\_epochs=400, nkerns=[96, 256, 386, 384, 256], batch\_size=20\\

MFSD  (learning\_rate=0.01, n\_epochs=400, nkerns=[96, 256, 386, 384, 256], batch\_size=20):\\
- svm\_gauss: Utilizando svm (rbf) como clasificador, con inizializacion gaussianana.\\
- svm\_genera:utilizando SVM (rbf) con inicializacion de pesos gaussiana \\
- svm\_linear: SVM (linear) con inicializacion de pesos gaussiana\\

IMPORTANT: The valid graphs are made with SOFTMAX independly of the classifier used to test.\\

\subsubsection{frav results}

In this section, cost (at training) and error (at validating) is going to be visualized. First when FRAV database has been trained with Gaussian initialization and with a learning rate = 0.001 \ref{fig:Imagenet2-frav1}. Second with the same learning rate, but Gaussian initialization for weights \ref{fig:Imagenet2-frav_gaussian_init}. Third, decreasing the learning rate to 0.01 and with normal initialization \ref{fig:Imagenet2-frav-svm_general} and the last one, whit the same learning rate but with Gaussian weights initialization \ref{fig:Imagenet2-frav-svm_gauss}.\\


\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/frav1/cost_frav.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/frav1/error_frav.png}
\caption{Cost at training and error at validating. Normal initialition. Learning rate = 0.001 (frav1).} \label{fig:Imagenet2-frav1}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/frav_gaussian_init/cost_frav.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/frav_gaussian_init/error_frav.png}
\caption{Cost at training and error at validating. Gassian initialization. Learning rate = 0.001 - frav\_gaussian\_init} \label{fig:Imagenet2-frav_gaussian_init}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/svm_gauss/cost.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/svm_gauss/error.png}
\caption{Cost at training and error at validating - Gaussian initialization. learning rate = 0.01 frav svm\_gauss.} \label{fig:Imagenet2-frav-svm_gauss}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/svm_general/cost.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/svm_general/error.png}
\caption{Cost at training and error at validating - Noraml initialization. learning rate = 0.01 frav svm\_general.} \label{fig:Imagenet2-frav-svm_general}
\end{figure}


First FRAV experiment (SOFTMAX as classifier and regular initialization) gives good results. The cost converges to 0 at training, the validation gets a 1.470588 \% best error rate with test performance of 5 \%. In testing, just 10 samples has been misclassified (7 samples of class 0 and 3 of class 1, from 679 test samples as total. The ROC  and precision-recall curves could been visualized figure \ref{fig:Imagenet2-frav-frav1ROC}\\

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/frav1/ROC.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/svm_general/Precision-Recall.png}
\caption{ROC and Precision- Recall courve - Noraml initialization. learning rate = 0.01 frav svm\_general.} \label{fig:Imagenet2-frav-frav1ROC}
\end{figure}

It could be seen in the graphics that the Gaussian initialization does not matter when the learning rate is 0.01, but the cost changes when the learning rate is 0.001. In this case, the learning rate 0.001 should be the chosen one.\\

In the table \ref{fravv} The positive and negative rates are visualized from the different classifier (softmax and SVM) the two different weights initialization and the two learning rates used. The results of the table are the same when the learning rate is 0.01 independently of the weight initialization or the kernel used to classify. The metrics rate are not too bad, with the learning rate the results are worse, the learning rate is too big.\\

\begin{table}[htb]
\centering
\label{fravv}
\begin{tabular}{|ccccccc|}
\hline
Classifier &  Weight initialization & learning rate & TP  & TN  & FP  & FN \\ \hline
Softmax    &         Normal        &     0.001     & 61  & 609 &  3  & 7  \\
Softmax    &       Gaussian        &     0.001     & 66  & 605 &  7  & 2   \\
SVM RBF(C=5)&         Normal       &     0.01      & 63  & 593 & 19  & 5  \\
SVM RBF(C=5)&         Gaussian     &     0.01      &  63 & 593 & 19  &5  \\
SVM lineal(C=5)&      Gaussian     &     0.01      &  63 & 593 & 19  &5  \\
\hline
\end{tabular}
\end{table}


\subsubsection{casia results}
For Casia, different experiments has been carried out in order to train the network, using SOFTMAX as classifier and normal weight initialization:\\

\begin{itemize}
\item{Test 1}: Learning rate = 0.01 y 400 epoch.
\item{Test 2}: Learning rate = 0.001 y 400 epoch.
\item{Test 3}: Learning rate = 0.0005 y 400 epoch.
\item{Test 4}: Learning rate = 0.001 y 1000 epoch.
\end{itemize}


In figure \ref{fig:lossCasia4Test} the cost at training could be visualized. In general, should decrease varying its value but decreasing logarithmically. In the first experiment (test 1), the value varies but in a small range, and in general it is constant, in the other three experiments, the cost decreases and this is what must happen, but in test 2 the loss converges two times, after converge the first time, then it increase again ans converges again.\\

In figure \ref{fig:errorCasia4Test} it is possible to see that the error changes but not in a logarithmically way. It increase and decreases its value in each epoch but it all experiments it gets in a 42\% or 40\% error. The desired curve should be as the loss one, but the error should not decrease as much as the training loss does.\\


\begin{figure}[htb]
\centering
		\subfigure[Test 1]{\includegraphics[width=0.47\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/prueba1/cost_frav_p1.png}}
		\subfigure[Test 2]{\includegraphics[width=0.47\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/prueba2/cost_frav_p2.png}}			\subfigure[Test 3]{\includegraphics[width=0.47\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/prueba3/cost_frav_p3.png}}
		\subfigure[Test 4]{\includegraphics[width=0.47\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/prueba4/cost_frav_p4.png}}  
\caption{Training loss of four test Casia database}  
\label{fig:lossCasia4Test}
\end{figure}

\begin{figure}[htb]
\centering
		\subfigure[Test 1]{\includegraphics[width=0.47\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/prueba1/error_frav_p1.png}}
		\subfigure[Test 2]{\includegraphics[width=0.47\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/prueba2/error_frav_p2.png}}			
		\subfigure[Test 3]{\includegraphics[width=0.47\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/prueba3/error_frav_p3.png}}
		\subfigure[Test 4]{\includegraphics[width=0.47\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/prueba4/error_frav_p4.png}}  
\caption{Validation error of four test Casia database}  
\label{fig:errorCasia4Test}
\end{figure}


As the same way that in the first experiment that FRAV database converges is that would be expected from Casia, but it has not gotten.\\

\clearpage
At the same way as FRAV, the classifier and the weight initialization has been changed in order to know how the networks behavior with these changes. The learning rate used for this experiment is 0.01. First, the cost (at training) and the error (at validating) are going to be visualized when the weight initialization is normal \ref{fig:Imagenet2-casia-svm_general}. Second when the weight initialization used is Gaussian \ref{fig:Imagenet2-casia-svm_gauss}.\\

%\begin{figure}[htb]
%\centering
%\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/%casia_gaussian_init/cost_frav.png}
%\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/%casia_gaussian_init/error_frav.png}
%\caption{Cost at training and error at testing - casia\_gaussian\_init} %\label{fig:Imagenet2-casia_gaussian_init}
%\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/svm_gauss/cost.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/svm_gauss/error.png}
\caption{Cost at training and error at validating -casia svm\_gauss.} \label{fig:Imagenet2-casia-svm_gauss}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/svm_general/cost.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/svm_general/error.png}
\caption{Cost at training and error at validating -casia svm\_general.} \label{fig:Imagenet2-casia-svm_general}
\end{figure}

As the same way than in FRAV database, the cost and at training the error at validating has not changed. The positive and negative rates are the following ones in the three experiments:  TP = 8; TN = 21; FP = 3; FN = 8.\\ 


\clearpage
\subsubsection{MFSD results}

%\begin{figure}[htb]
%\centering
%\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/mfsd/mfsd_gaussian_init/cost_frav.png}
%\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/mfsd/mfsd_gaussian_init/error_frav.png}
%\caption{Cost at training and error at testing - mfsd\_gaussian\_init} %\label{fig:Imagenet2-mfsd_gaussian_init}
%\end{figure}

As the same way in FRAV and CASIA, but now with MFSD, it is going to be tested the network with a learning rate of 0.01, Gaussian initialization \ref{fig:Imagenet2-mfsd-svm_gauss} and normal distribution initialization \ref{fig:Imagenet2-mfsd-svm_general} and classifying the test with SVM (RBF kernel and linear) and softmax.\\

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/mfsd/svm_gauss/cost.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/mfsd/svm_gauss/error.png}
\caption{Cost at training and error at validating - mfsd svm\_gauss.} \label{fig:Imagenet2-mfsd-svm_gauss}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/mfsd/svm_general/cost.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/mfsd/svm_general/error.png}
\caption{Cost at training and error at validating - mfsd svm\_general.} \label{fig:Imagenet2-mfsd-svm_general}
\end{figure}

The same problem occurs. The train and valid graphs are the same in both cases, independently of the initialization.\\ Also, the result at testing is the same in three cases: TP = 12, TN = 3, FP = 105, FN = 0\\


\clearpage 

\subsection{New database}
From images, a new database has been developed. Images are read and re-sized directly to 128x128. The reason of using 128x128 is the size that authors in the Casia paper use.  The databases explained in \ref{tabla-databasesDistribution}:

\begin{table}[htb]
\centering
\begin{tabular}{cccccc}
-                         & FRAV & FRAV (rgb+nir) & CASIA images & CASIA video &  MFSD\\
nº train samples class 0  & 157  &   133    &     26       &  255    &    30\\
nº train samples class 1  & 459  &   417    &     111      &  81     &    68\\
nº test samples class 0   &  19  &   16    &       16      &  540    &     3\\
nº test samples class 1   & 167  &   141    &      23      &  180    &    25 \\
nº valid samples class 0  &  10  &   8     &        7      &  105    &     2\\
nº valid samples class 1  &  83  &   70    &       13      &  39     &    12\\

\end{tabular} \label{tabla-databasesDistribution}

\end{table}


I can not obtain results with Casia RGB + NIR appended in classifier because it said that there is not space enough.\\

\clearpage
\subsubsection{experiments}
With that databases. Some experiments has been carried out:

\begin{itemize}
\item general experiment, with Gaussian weight initialization, classification with SVM RBF and SOFTMAX.
\item general experiment but with a small database in order to make over-fitting in the network and check it. It has been used 20 train, test and validation images in all databases but MFSD that has been used 14 images for each subset.
\item The same experiment that above but the test has been realized with the same subset that in training, this is to check that the network has over-fit or should have over-fitted.
\item The same as above but decreasing the learning rate from 0.01 to 0.001. \\
\end{itemize}


\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_frav/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_frav/error.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_frav/minidataset/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_frav/minidataset/error.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_frav/minidataset_tested_itself/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_frav/minidataset_tested_itself/error.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_frav/minidataset_tested_iteself_lr_0_001/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_frav/minidataset_tested_iteself_lr_0_001/error.png}
\caption{cost and error of the tree experiments with frav.} \label{fig:frav-ejec1}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_frav_rgb_nir/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_frav_rgb_nir/error.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_frav_rgb_nir/minidataset/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_frav_rgb_nir/minidataset/error.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_frav_rgb_nir/minidataset_tested_itself/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_frav_rgb_nir/minidataset_tested_itself/error.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_frav_rgb_nir/minidataset_tested_iteself_lr_0_001/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_frav_rgb_nir/minidataset_tested_iteself_lr_0_001/error.png}
\caption{cost and error of the tree experiments with FRAV (rgb + nir) image level images.} \label{fig:frav_imagelevel-ejec1}
\end{figure}


\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_casia/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_casia/error.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_casia/minidataset/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_casia/minidataset/error.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_casia/minidataset_tested_itself/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_casia/minidataset_tested_itself/error.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_casia/minidataset_tested_iteself_lr_0_001/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_casia/minidataset_tested_iteself_lr_0_001/error.png}
\caption{cost and error of the tree experiments with CASIA images.} \label{fig:casia-ejec1}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_casia_video/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_casia_video/error.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_casia_video/minidataset/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_casia_video/minidataset/error.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_casia_video/minidataset_tested_itself/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_casia_video/minidataset_tested_itself/error.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_casia_video/minidataset_tested_iteself_lr_0_001/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_casia_video/minidataset_tested_iteself_lr_0_001/error.png}
\caption{cost and error of the tree experiments with CASIA videos.} \label{fig:casiavid-ejec1}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_mfsd/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_mfsd/error.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_mfsd/minidataset/minidatasetcost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_mfsd/minidataset/minidataseterror.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_mfsd/minidataset_tested_itself/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_mfsd/minidataset_tested_itself/error.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_mfsd/minidataset_tested_iteself_lr_0_001/cost.png}
\includegraphics[width=0.45\textwidth]{images/redes/ejecucion1/general_svm_mfsd/minidataset_tested_iteself_lr_0_001/error.png}
\caption{cost and error of the tree experiments with MFSD images.} \label{fig:mfsd-ejec1}
\end{figure}

From images, could be concluded that in general, decreasing the learning rate for this experiment has not been a good idea.\\

In table \ref{table-ej1} could be seen the positive and negatives rates when has been used SVM RBF to classify. The result obtained with FRAV (rgb + NIR) is really good because just 3 samples have been missclaffied from 140 images.\\

I do not know why the test is the same for minidataset and minidatset tested with itself.\\

\begin{table}[htb]
\centering
\label{table-ej1}
\begin{tabular}{cccccc}
-              &Optima CSVM& TP & TN & FP & FN \\
FRAV           &    0.05   & 136& 24 &  11 & 9 \\
FRAV (rgb+nir) &    0.1    & 113& 24 &  1  & 2 \\
CASIA images   &    5      & 9  & 2  &  0  & 9 \\
CASIA videos   &    0.1    & 478& 75 &  105& 62 \\
MFSD           &    10     & 19 &  1 &   8 & 0 \\
\end{tabular}
\end{table}



Looking the graphs where a minidataset has been used (20 images or 14), if the cost (training) is visualized, could be expected that the train is learning the images because the cost decreases to 0 (almost zero) so that means that if it is tested with itself the error should be 0, but that does not happen.\\

The conclusion is the needed of a balanced database, at least to do this experiment. In which the number of class 0 samples are the same that the number of class 1 samples, because the network would not learn in the same way if in some cases the number of samples of  attack class is four times than the number of samples of class 1, just predicting 0 would have 25\% accuracy, and having less than 5 samples in validation or testing is not a good generalizer (2 samples in class 0 MFSD database).\\ 


