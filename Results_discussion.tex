%!TEX root = Memoria_TFM.tex
%\minitoc
%\mtcskip
\begin{small}
\emph{In this chapter, from results obtained in previous chapter (chapter \ref{ch:results}) is detailed with the purpose of achieve the conclusion of the following chapter.\\}
\end{small}

\section{Discussion of LeNet-5 and its results}
The final architecture has been developed from LeNet-5 architecture whose training, validation and test performance is exemplary as it has been explained in \ref{sec:lenet_results}.\\

The large size of the batch size used in LeNet-5 architecture it has been changed to test how the learning process is modified, because in the experiments made with others databases, the amount of samples is not enough to preserve the batch size. The used learning method is the Mini-batch Stochastic Gradient Descend, it uses the samples in each batch to learn. And results obtained are improved (when batch size is 100) and descend when batch size used is 20. \\

Usually, in literature, a 32 size is used. And it has been concluded that the batch size is used not to provide the network with all the samples, because the computational resources are limited and depending on the batch size, the neural network would need more epochs.\\

Due to the fact that the Lenet-5 architecture is optimized for MNIST Digit classification, when changes have been made (changing weight initialization, modifying activation function, adding ReLu layer, etc.) results have not been improved, although the difference is not immense.\\

The worst result is when Gaussian weight initialization is used, the performance of the training process suggest that the learning is in a local minimum, because the training performance is correct, however the validation and the test processes return a high error rate.\\

Regarding to using RGB FRAV database with LeNet-5 architecture (detailed in section \ref{Lenet-FRAV}), the purpose of the experiment is test LeNet-5 with a anti-spoofing face database. The  results obtained are not satisfying. In that experiment, RGB FRAV database has been testing using two classes (attacks and genuine users) and five classes (genuine users and each attack correspond with one different class).\\

In both cases the results are poor and the reason is the architecture which is simple and is not able to extract relevant features from images in order to classify them correctly, there is not a learning curve process, the architecture required to be modified; indeed, the learning curve suggest that the learning rate is small because the cost does not decrease, there is no learning.

\section{Discussion of the own architecture developed experiments}
In consequence with the poor performance of LeNet-5 with the anti-spoofing database, the architecture has been modified (described in section \ref{sec:adapt_lenet}). Previous works are based on \textit{Imagenet} architecture and its results are respectable, therefore an own architecture based on Imagenet is developed.\\

The four experiments described to test the architecture behaviour, have been realized with the three anti-spoofing databases: CASIA, FRAV and MFSD-MSU databases.\\

The training process of the general experiment are acceptable because each one converges into a minimum and close to 0, meaning that the trianing procedure is correct, although there are oscillations and the cost curve is not as perfect as the obtained with original LeNet-5.\\

The validation process should decrease its value in each epoch converging in a low value (not as lower as the obtained with the cost performance). The validation performance obtained with CASIA image database is not the desirable one because it oscillates changing the error value between 60\% and 10\%. The validation curve obtained in CASIA video, RGB FRAV database and RGB+NIR (feature level) is preferred, the value descends until converge.\\

The result obtained in RGB from RGB+NIR (classification level) FRAV database suggest an overfitting, because the cost descends until 0 and the error at validation descends until 0\% and then improve until converge in a 10\%, what means that the training samples are being learnt.\\

The validation result obtained with MFSD-MSU does not show a favourable performance, the value oscillates between 30\% and 40\% which are large error values.\\

The cost results obtained with experiment two and three are as expected, the cost converges in 0 for each database. Despite the fact that the validation error obtained in experiment 2 and experiment 3 is not a desirable result: for each database the error oscillates sharply, experiments 3 converges in 0\% error for all databases except for FRAV one (overfitting has been provoked). Experiment 2 result does not converge in low values.\\

The results obtained in experiment four are not acceptable for any database. The training cost and the validation error get constant values. Decreasing the learning rate is not an option for any database. The training cost curve should decrease slower and converge.\\

\section{Discussion of the final experiment}
The final architecture built is tested with the all the databases (as is detailed in \ref{sec:Final_archi}).\\


