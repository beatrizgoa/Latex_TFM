%!TEX root = Memoria_TFM.tex
\minitoc
\mtcskip

\begin{small}
\emph{In this chapter the start and the evolution of the built convolutional neural network used is described.\\}
\end{small}

\section{LeNet-5}
LeNet-5 \cite{Lenet5} is the name of a certain architecture of a convolutional network designed for document recognition (handwritten, machine printed characters) developed by Yan Lecun \textit{et al}.\\

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.7\textwidth]{images/images_lenet/LenetArquitectura.png}
  \caption{LeNet-5 Arquitecture}
  \label{Lenet5Arquitectura}
\end{figure}

The basic architecture of LeNet-5 is two convolutional layer, followed each one by a max pooling layer and then a fully-connected layer. This architecture could be visualized in figure \ref{Lenet5Arquitectura} where it is possible to visualize the input image dimensions across the layers and its final shape.\\

LeNet is a useful convolutional neural network that is usually used by beginners users to learn deep learning matter because its short architecture and it is implemented in lots of deep learning framework using it to explain the framework. Because of this, LeNet-5 has been used as basis of the project and to learn Theano and convolutional neural networks theory and implementation.\\

The code of LeNet in Python using Theano library, and its explanation, is openly available in \url{www.deeplearning.net}.\\

\subsection{LeNet-5 specifications}
The specifications of the downloaded LeNet code are the architecture of LeNet-5 used for starting to work with this project is formed by two convolutional layers of size 5x5 and with 20 kernels in the first convolutional layer and 50 in the second one, those are followed (each one) by a max pooling-layer of size 2x2. Those four layer are followed by a fully-connected layer with 500 neurons at the output.\\

The classifier which has been used is the logistic regression which is trained at the same time as the convolutional neural network. The activation function of the convolutional layers and the logistic regression is tanh. The learning rate used is 0.1 and the network runs by 200 epoch.\\

The cost function or loss that must be minimized during the training is the negative log-likelihood. LeNet-5 uses the stochastic gradient method with mini-batches (MSGD).\\

The data used is MNIST digit database, whose characteristics are described in section \ref{subsec:MNIST}. The data comes split in three subsets: training, testing and validating. Each subset is used for training, testing or validating respectively.\\

The data is not fed to the network in one go, each subset is grouped in smalls subsets called batches and whose size is chosen by user. In the code available of LeNet-5 the batch size is 500 samples. The network is fed by batches, so the size of the net depends on the batch size not the (train, test or validate) subset. In this example, the batch size, is the same for the three subsets. When the subset is divide into batches, if there are some samples that are not enough for a batch, those samples are not used.\\

The network train for a specified number of epoch. Each epoch has as many iterations as necessary to go through all batches of the train subset. The reason of using batches is to define the size of the network and because, usually, the quantity of samples used in deep learning is big (thousand, millions..) and too much memory would be need to build a network of its size and the computational resources available may not be enough.\\

As the MNIST digit database available with the code has 50000 samples for training, 10000 for testing and 10000 for validating, the number of batches for each subset (with 500 samples for each batch) is 100 for training, 20 for testing and 20 for validating

The training procedure is being realized for too many epochs as users has selected and returns the cost of the procedure. While the trianing is being running, the validation is calculated for each epoch and the validation returns the error of the procedure. The training cost and the validation error are used to know the behavior or the learning process of the network and how it generalizes with the purpose of choosing the best model.\\

The test is realized while the training is being executed, more specifically, when the validation has been realized and the results are the best obtained in the whole process until that iteration. The result that is used to compared with others classifiers or with others articles.\\

In addition to the number of epoch, other way to stop the training procedure is early-stopping, this method is used to avoid over fitting tracking the validation process \cite{Yoshua}. The decision of stopping the training depends on \textit{the patience} and is chosen by user.\\

%%%%% Creo que esto de aqui abajo no es necesario %%%%%%
%First of all in Lenet the data is loaded. The function that download the data check, firstly, if the data is not downloaded yet, if it downloaded it does not repeat the download. The data is downloaded split into train, test and validation subsets.\\

%After loading the data, the architecture is defined, the layers are called because they has been defined as objects, each kind of layer, one different object. There is a object for convolutional + maxpooling, another object for the hidden layers that is a fully connected layer, and the classifier used that is logistic regression. Each part of the code could be found in \url{www.deeplearning.net}\\

%After creating the structure, the functions of training, validating and testing are created as Theano functions.\\

%In a big while loop the training validation and testing is developed. The data is trained. Each mini-batch (that is a small quantity of that given by the user, it is used in order to not train or test all the data together because it would take too much computer resources) is trained and the weights and bias of each layer are updated. It is given a validation frequency, this parameters decides how many validations are produced. So the data is trained and validation, when a best score of validation is produced, the data is tested.\\

%The training would run for a number of epoch that the user has given or by a early-Stopping that has been developed by authors. The early-stopping combats over-fitting by monitoring the model's performance on a validation set.\\

\subsection{LeNet-5 Results}
While the training is being calculate, the weights are being update in each iteration. When A  model is selected or saved, weights are actually what is being chosen or saved. An example of weights is represented in figure \ref{fig:weights_lenet} where twenty first weights at epoch 10 of the first convolutional layer are represented. So when its being trained, what the network is doing is adapting the weights to the input to get a good performance.\\

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{images/images_lenet/w_layer0_epoch10.png}
\caption{Weights at epoch 10 of the first convolutional layer} \label{fig:weights_lenet}
\end{figure}

The training procedure returns the cost function in each iteration to evaluate the training behavior. The cost function at training obtained executing LeNet-5 is represented in figure \ref{fig:Lenetcost}, and its value decreases as the iterations rise converging in almost 0; this curve is the desired one for each training practice, because it is not oscillate abruptly and converges in a very low value logarithmically.\\

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{images/images_lenet/cost_lenet.png}
\caption{Cost function at training running LeNet-5 with MNIST digit database.} \label{fig:Lenetcost}
\end{figure}

The error obtained at validation is represented in figure \ref{fig:Lenetresult}, where could be seen that the value decreases logarithmically too converging, approximately, in 1 (a low value) and this behavior of the curve is the desired in a validation process. The convergence value of the validation is not usually as much lower as the training convergence value, and the point where it starts to converge is later than in the training.\\

\begin{figure}[htb]
\centering
\includegraphics[width=0.7\textwidth]{images/ModificandoLenet/error_lenet.png}
\caption{Validation error obtained with LeNet-5 with MNIST digit database} \label{fig:Lenetresult}
\end{figure}

The test result has been calculated with the model of the iteration 18300 (epoch $37^{th}$), with a validation error of 0.91\%. The error rate obtained is 0.92\% what it means that 920 samples of 100000 of the testing subset are being misclassified.\\

%The neural network has beoen built in Python using Theano library. The %network has been based on the one exposed in \textit{Learn Convolutional %Neural Network for Face Anti-Spoofing} and \textit{LeNet}.\\
%The architecture of the network is formed by two convolutional layers which %is followed (each one) by a max-pool layer.The  last max-pool layer is %followed by a fully-connected layer which have sigmoidal activation %function.\\
%The rectified linear activation function (reLu) is used as activation %function of the convolutional layers. It has been used a normalized %distribution of weights and bias, the same which was implemented in LeNet: %weights are sampled randomly from a uniform distribution in the range [-1/%fan-in, 1/fan-in], where fan-in is the number of inputs of the previous %layer. The ReLu function that has been used is the one implemented in %theano as theano.tensor.nnet.relu.\\
%The number of kernels used in the conv layers are 48 and 96 of size 5x5, %the size of the max-pool layer is 2x2.\\
%The network is trained with the 70\% of the data, the 30\% is used to test. %From the 70\% of the training data, the 25\% is used to validation.\\

\subsection{Modifying LeNet}
Modifications have been made to LeNet-5 architecture. First the batch size has been changed and then the activation function, a normalization layer has been added and the weight initialization has been changed too. The database used for those experiments is the MNIST digit database.\\

\subsubsection{Changing the batch size}
Two experiments have been developed in order to compare the results when the batch size is changed and how the network behaviour change too with respect to LeNet-5 with the original batch size (500).\\

The first experiment is with 20 samples per batch. For the second experiment, the batch size used is 100. In both cases, the training process has been stopped by the early-stopping; at epoch $31^{th}$ has stopped the first experiment, because from epoch $16^{th}$ the error at validating was not being improved and for the second experiment, at $33^{th}$ epoch is when the early-stopping has finished the training.\\

The validation error for both experiments and the original LeNet are represented in figure \ref{figures:LENET-batches}. From images could be seen that the validation error in first epochs is lower (9 \% in the first experiment) than the validation error when the batch size is bigger. When the batch size is equal to 20, the optimal test error rate has been obtained in the first $15^{th}$ epochs, the same error rate than in the original case (0.92\%), but for 100 samples per batch, it has not been possible to get to that error rate, the best test error rate has been 1.04\% at iteration 8500.\\

\begin{figure}[htb]
    \centering
	\subfigure[batch size = 500 (original size)]{\includegraphics[width=0.47\textwidth]{images/images_lenet/lenet_batch/lenet_500batch_rror.png} }
	\subfigure[batch size = 20]{\includegraphics[width=0.47\textwidth]{images/images_lenet/lenet_batch/lenet_20batch_error.png} }
	\subfigure[batch size = 100]{\includegraphics[width=0.47\textwidth]{images/images_lenet/lenet_batch/lenet_100batch_error.png} }

    \caption{Validation error in each epoch for different sizes of batches.} \label{figures:LENET-batches}
\end{figure}

%Each time we take a sample and update our weights it is called a mini-batch. Each time we run through the entire database, it's called an epoch. Batch size determines how many examples you look at before making a weight update. The lower it is, the noisier the training signal is going to be, the higher it is, the longer it will take to compute the gradient for each step. \url{http://stats.stackexchange.com/questions/140811/how-large-should-the-batch-size-be-for-stochastic-gradient-descent}.\\

Concluding, more epochs are necessary when the batch size is bigger because there are not enough updates in each epoch \cite{Yoshua}. Usually, the value of the batch size used is 32 \cite{Yoshua} and the choice, generally, is computational.\\

In figure \ref{figures:LENET-batches} the error in each epoch is represented  for 500 batch size, the original size, for a value of 20 and 100. In the original case, the error stars with a value of 9\% approx. with the batch size = 20 the error in the first iteration is about 2.4\%, and with a bunch of 100 images, the validation error is 3.5\%.\\

With the original size and size equal to 20, it is possible to get to the same minimum, the difference between those examples is that each one get to that conclusion into different epochs. With a batch size equal to 100, the code stopped because of the early-stop with a patience of 10000; it stop in epoch 33,while those epochs, it has been possible to get to a test error of 1.04\% in iteration 8500 when the validation error was 1.01\%, it has been running with put getting a better validation score for 17 epoch. With a batch size = 20, the code also has stopped earlier because of the same reason, but in that case it has been possible to get to the same minimum that with the original size; the epoch in which has stopped is 31, it has been running without getting a better validation score for 15 epochs.\\


%If the patience is increased from 10000 to 100000 for a batch size = 20 and equal to 100, results obtained are that for a value of 20, it has been running for 40 epoch, having the same results that the one obtained with a patience of 10000.\\
%For a value of 100 with the patience increased to 100000, the results are not the same as the one obtained the patience equal to 10000. In this occasion, the results are better than in the original one; The code has run for 258,08 minutes and for 200 epochs. The result obtained was better than in the original one has it has said. In epoch 51 has gotten a validation error of  1 \% and the error test has been 0,89\%, from this point, the test error that has been calculated five more times, has been increasing until get the value 0,85\% in iteration 55500, epoch number 111, the validation error obtained has been 0,95\%. From this epoch until number 200, the network has not had any better result.\\
%In figure X it is possible to see the error function for those two examples. It could be seen that the example with batch size = 20 has just run for 40 epoch and the good results of the example with batch size = 100, remembering that the patience has been increased from 10000 to 100000.\\


\subsubsection{Changing activation function, normalization and weights initialization}
As the same way as the batch has been changed and its results has been compared in the previous subsection, the activation function has been changed, a normalization layer has been added and weights initialization has been changed.\\

LeNet-5 does not use any normalization layer, but in this experiment from the different normalization availables (batch normalization, local normalization, ...) Local Response normalization has been added after the max-pooling layers.\\

The activation function used in LeNet-5 is tanh, it has been changed to rectified linear unit (ReLu) activation function.\\

With respect to the weight initialization, in LeNet, for the convolutional layers and the fully connected layer,  a normalized initialization \cite{XavierInitialization} is used:

\begin{equation}
  W \sim U [- \frac{\sqrt{6}}{\sqrt{n_{j}+n_{j+1}}},\frac{\sqrt{6}}{\sqrt{n_{j}+n_{j+1}}}]
\end{equation}

Where $n_{j}$ is the number of neuron of the current layer and $n_{j}$ is number of neurons of the following layers. In this experiment, this initialization has been changed to a weight initialization with a Gaussian distribution.\\

Below the details of each experiment are described:\\

\begin{itemize}
\item{Experiment 1: using Local Response Normalization (LRN) In which a normalization has been carried out in the convolutional-max pooling layers.}
\item{Experiment 2: using ReLu as activation function:} The activation function tanh has been substituted by ReLu activation function in convolutional and fully connected layers.
\item{Experiment 3: using ReLu and LRN}: The activation function used is ReLu and LRN has been used as  normalization layer.
\item{Experiment 4: changing weights initialization}: Weights initialization has been changed by Gaussian. In which mean value that has been used is 0 and std is 0.01. Weights initialization has been changed in convolutional and fully connected layers. Also, bias initialization has been changed by ones.
\end{itemize}

%Using Local Response Normalization we want to detect high frequency features with a large response. If we normalize around the local neighborhood of the excited neuron, it becomes even more sensitive as compared to its neighbors.[\url{https://prateekvjoshi.com/2016/04/05/what-is-local-response-normalization-in-convolutional-neural-networks/}].\\

First, the cost of training process are going to be visualized with the original cost train, without modifying LeNet). In figure \ref{fig:CostModLeNet} is represented. Also the validation error ($validation_error = validation_cost*100$) could be visualized in figure \ref{fig:CostModLeNet}. The network has been running for 200 epochs.\\

\begin{figure}[htb]    \centering
	\subfigure[Original LeNet]{\includegraphics[width=0.42\textwidth]{images/ModificandoLenet/cost_lenet.png}\label{fig:cost_lenet} }
	\subfigure[Experiment 1: using LRN]{\includegraphics[width=0.42\textwidth]{images/ModificandoLenet/cost_lenet_LRN.png}}
	\subfigure[Experiment 2: using ReLu]{\includegraphics[width=0.42\textwidth]{images/ModificandoLenet/cost_lenet_relu.png}}
	\subfigure[Experiment 3: using ReLu and LRN]{\includegraphics[width=0.42\textwidth]{images/ModificandoLenet/cost_lenet_relu_lrn.png}}
	\subfigure[Experiment 4: using Gaussian weights init.]{\includegraphics[width=0.42\textwidth]{images/ModificandoLenet/cost_lenet_gausianInit.png} \label{fig:cost_gaus_lenet}}
    \caption{Cost function of Lenet and Lenet Modified.} \label{fig:CostModLeNet}
\end{figure}

Visualizing the loss during the training, could be affirmed that the network with Gaussian initialization is in a local minimum because the loss has converged as could be seen in figure \ref{fig:cost_gaus_lenet}. The loss of LeNet-5 without being modified (figure  \ref{fig:cost_lenet}) is the one whose oscillation at training is less than others.

\begin{figure}[htb]    \centering
	\subfigure[Original LeNet]{\includegraphics[width=0.42\textwidth]{images/ModificandoLenet/error_lenet.png}\label{fig:error_lenet} }
	\subfigure[Experiment 1: using LRN]{\includegraphics[width=0.42\textwidth]{images/ModificandoLenet/error_lenet_LRN.png}}
	\subfigure[Experiment 2: using ReLu]{\includegraphics[width=0.42\textwidth]{images/ModificandoLenet/error_lenet_relu.png}}
	\subfigure[Experiment 3: using ReLu and LRN]{\includegraphics[width=0.42\textwidth]{images/ModificandoLenet/error_lenet_relu_lrn.png}}
	\subfigure[Experiment 4: using Gaussian weights init.]{\includegraphics[width=0.42\textwidth]{images/ModificandoLenet/error_frav_gausianInit.png} \label{fig:error_gaus_lenet}}
    \caption{Valid error of Lenet and Lenet Modified.} \label{fig:ErrorModLeNet}
\end{figure}

About the error at validation, visualizing the graphs in figure \ref{fig:CostModLeNet}, it is very similar the curve for original LeNet-5, LeNet-5 with ReLu, LeNet-5 with LRN and LeNet-5 with LRN and ReLu.\\
\clearpage

The results obtained, at testing, have been the following ones:

\begin{itemize}
\item{Original LeNet}: Best validation score of 0.91 \% obtained at iteration 17400, with test performance 0.92\%.
\item{Experiment 1: using Local Response Normalization}: Best validation score of 0.99 \% obtained at iteration 13400, with test performance 1.6 \%.
\item{Experiment 2: using ReLu as activation function}:Best validation score of 1.04 \% obtained at iteration 11900, with test performance 2.4\%.
\item{Experiment 3: using ReLu and LRN}: Best validation score of 1.18 \% obtained at iteration 19500, with test performance 1.08 \%.
\item{Experiment 4: Gaussian weight initialization}: Best validation score of 81.22\% obtained at iteration 100, with test performance 80.90\%.
\end{itemize}

The best configuration for the network is the original one. With Gaussian initialization, the network does not find a local minimum in such a sort of time. Using LRN and ReLu, test result is closer to the obtained with LeNet-5 original, but not as good as the last one. Changing the activation function has not been a good change. Not taking into account original LeNet-5, the best test performance has been obtained with 1,08\% using ReLu and LRN, but the best validation error is 0,99\% obtained using just LRN. The values are close of the modifications, but the modification of Gaussian weight initialization.\\

\subsection{LeNet-5 and RGB FRAV faces database}
The goal of this thesis is train a convolutional neural network for face anti-spoofing, so the following step is feed LeNet-5 with one face databases, RGB FRAV database, the one that would be used in the final architecture.\\

One of the databases used with the final architecture and configuration of the network is FRAV database. LeNet-5 is tested with this database in this current section.\\

In order to work with images, because of the difference among the images shape, they have been resized into 252x180, this new shape is proportional to $0.7*height = weight$ because all images studied save that proportion approximately.\\

The network has been tested with this databases in the two ways of classify images, with two classes (genuine and attacks) and five classes (genuine and four classes, one per type of attack).\\ %Also, different learning rates have been used.\\

The architecture used is the same as LeNet-5 except for the batch size that has been reduced to 50 because there are not as much samples as in the MNIST database.\\

\begin{figure}[htb]
\centering
\subfigure[Cost]{\includegraphics[width=0.47\textwidth]{images/ModificandoLenet/lenet_frav_cost.png}\label{Lenet_FRAV_cost} }
\subfigure[Error]{\includegraphics[width=0.47\textwidth]{images/ModificandoLenet/lenet_frav_error.png} \label{Lenet_FRAV_error} }
\caption{Cost at training (a) and Error at validation (b) when LeNet has been used with RGB FRAV database.}
\label{fig:Lenet_FRAV}
\end{figure}

In figure \ref{fig:Lenet_FRAV} is represented the training cost \ref{Lenet_FRAV_cost} and the validation error \ref{Lenet_FRAV_error} of the training process. In the figure, is represented when 2 classes are used to classify in  blue and red when 5 classes are used, for both cases (cost and error). From the figure it is possible to visualize that the hole training process is not desirable for 2 classes and 5 classes, because it seems that the network does not learn, the architecture should be modified.\\

\section{Adapting LeNet-5 architecture}
From LeNet-5 architecture, changes have been made in order to build a similar convolutional architecture as described in \cite{yangLL14}. LeNet-5 architecture is simple to use it for this project purpose.\\

In \cite{yangLL14} authors develop a convolutional neural network with similar objectives as the described in this thesis. The CNN is based on \textit{Imagenet} architecture \cite{imagenet}, this is a determined architecture used for classifying objects which is well known and very used in the deep learning community. Its architecture is described in figure \ref{fig:Imagenet_architecture}.\\

\begin{figure}[htb]
\centering
\includegraphics[width=0.65\textwidth]{images_miscelaneus/Imagenet.png}
\caption{Imagenet architecture.} \label{fig:Imagenet_architecture}
\end{figure}

Imagenet is formed by convolutional layers, max-pooling layers, normalization layers, dropout layers and a fully-connected layer.\\

Authors from \cite{yangLL14} describe lightly the architecture, the parameters and the changes made from Imagenet, so it is not possible to develop the same architecture.\\

%Dentro de la carpeta de FRAv\_casia\_ImageNet, en imagenet2, se ha programado imagenet, lo unico que sin strides en la primera capa de convolucion.\\
%Because of the lack of information about the convolutional neural network described in casia paper, it has been necessary  to though the original code that authors use, Imagenet. Authors use the same architecture, although how it has been modified it is not explained. \\
%It has been possible to implement Imagenet, the difference between the implementation and the original one is that the strides used in the first convolutional layer has not been used because the input of the images need to be bigger to have more than one neuron in the last layer.\\
%Dentro hay una carpeta para cada base de datos. Dentro de cada carpeta estan las pruebas que se hacen con cada base de datos.\\
%The convolutional neural network has been tested with different databases: FRAV, CASIA and MFSD and different experiments have been carried out with each one.\\


\subsection{Experiments description} %IMAGENET 2
The goal of the next experiments is visualized the new architecture behaviour and make changes and how the network is adapted.\\

FRAV database (Common parameters:  n\_epochs=400, nkerns=[96, 256, 386, 384, 256], batch\_size=20):\\
\begin{itemize}
\item frav1: Gaussian weights initialization. SOFTMAX used as classifier. Learning rate = 0.001 Figure \ref{fig:Imagenet2-frav1}.
\item  frav\_gaussian\_initinizialization: Gaussian weight Initialization. Learning rate = 0.001. SOFMTAX used as classifier. Figure \ref{fig:Imagenet2-frav_gaussian_init} .
\item svm\_gauss: Using SVM (with RBF kernel) as classifier. Gaussian weight Initialization. Learning rate = 0.01 \ref{fig:Imagenet2-frav-svm_gauss}.
\item svm\_genera: Using as a classifier SVM with RBF kernel. Learning rate = 0.01 \ref{fig:Imagenet2-mfsd-svm_general}.
\item svm\_linear: Classifying with SVM (linear) and Gaussian weight initialization. Learning rate = 0.01\\
\end{itemize}

CASIA (images) ( nkerns=[96, 256, 386, 384, 256]):\\
First, four test has been carries out (in which the learning rate has been changed and the number of epochs. trying to get the best learning rate configuration. In four test the batch size used is 25 samples, the classifier used at testing is Softmax and the weight initialization used is normal distribution:

\begin{itemize} [noitemsep,topsep=8pt,parsep=0pt,partopsep=20pt]
\item Test1: learning rate = 0.01 and nº epochs = 400.
\item Test2: learning rate = 0.001 and nº epochs = 400.
\item Test3: learning rate = 0.0005 and nºepochs = 400.
\item Test4: learning rate = 0.001 and nº epochs = 1000,
\end{itemize}

It has not been possible getting a train loss that converge in a minimum in none of the four tests. A good train loss should decrease in each epoch until converge in a minimum. But in the tests, the
- casia\_gaussian\_init: gausiana de pesos con mean 0 y std 0.01. SOFMTAX como clasificador. learning\_rate=0.01, n\_epochs=400, nkerns=[96, 256, 386, 384, 256], batch\_size=20 \\
- svm\_gauss: Utilizando svm (rbf) como clasificador, con inizializacion normal. learning\_rate=0.01, n\_epochs=400, nkerns=[96, 256, 386, 384, 256], batch\_size=20\\
- svm\_general: utilizando SVM (rbf) con inicializacion de pesos gaussiana. learning\_rate=0.01, n\_epochs=400, nkerns=[96, 256, 386, 384, 256], batch\_size=20\\
- svm\_linear: SVM (linear) con inicializacion de pesos gaussiana. learning\_rate=0.01, n\_epochs=400, nkerns=[96, 256, 386, 384, 256], batch\_size=20\\

MFSD  (learning\_rate=0.01, n\_epochs=400, nkerns=[96, 256, 386, 384, 256], batch\_size=20):\\
- svm\_gauss: Utilizando svm (rbf) como clasificador, con inizializacion gaussianana.\\
- svm\_genera:utilizando SVM (rbf) con inicializacion de pesos gaussiana \\
- svm\_linear: SVM (linear) con inicializacion de pesos gaussiana\\

IMPORTANT: The valid graphs are made with SOFTMAX independly of the classifier used to test.\\

\subsubsection{Results with FRAV database}
In this section, cost (at training) and error (at validating) is going to be visualized. First when FRAV database has been trained with Gaussian initialization and with a learning rate = 0.001 \ref{fig:Imagenet2-frav1}. Second with the same learning rate, but Gaussian initialization for weights \ref{fig:Imagenet2-frav_gaussian_init}. Third, decreasing the learning rate to 0.01 and with normal initialization \ref{fig:Imagenet2-frav-svm_general} and the last one, whit the same learning rate but with Gaussian weights initialization \ref{fig:Imagenet2-frav-svm_gauss}.\\

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/frav1/cost_frav.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/frav1/error_frav.png}
\caption{Cost at training and error at validating. Normal initialition. Learning rate = 0.001 (frav1).} \label{fig:Imagenet2-frav1}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/frav_gaussian_init/cost_frav.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/frav_gaussian_init/error_frav.png}
\caption{Cost at training and error at validating. Gassian initialization. Learning rate = 0.001 - frav\_gaussian\_init} \label{fig:Imagenet2-frav_gaussian_init}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/svm_gauss/cost.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/svm_gauss/error.png}
\caption{Cost at training and error at validating - Gaussian initialization. learning rate = 0.01 frav svm\_gauss.} \label{fig:Imagenet2-frav-svm_gauss}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/svm_general/cost.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/svm_general/error.png}
\caption{Cost at training and error at validating - Noraml initialization. learning rate = 0.01 frav svm\_general.} \label{fig:Imagenet2-frav-svm_general}
\end{figure}


First FRAV experiment (SOFTMAX as classifier and regular initialization) gives good results. The cost converges to 0 at training, the validation gets a 1.470588 \% best error rate with test performance of 5 \%. In testing, just 10 samples has been misclassified (7 samples of class 0 and 3 of class 1, from 679 test samples as total. The ROC  and precision-recall curves could been visualized figure \ref{fig:Imagenet2-frav-frav1ROC}\\

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/frav1/ROC.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/svm_general/Precision-Recall.png}
\caption{ROC and Precision- Recall courve - Noraml initialization. learning rate = 0.01 frav svm\_general.} \label{fig:Imagenet2-frav-frav1ROC}
\end{figure}

It could be seen in the graphics that the Gaussian initialization does not matter when the learning rate is 0.01, but the cost changes when the learning rate is 0.001. In this case, the learning rate 0.001 should be the chosen one.\\

In the table \ref{fravv} The positive and negative rates are visualized from the different classifier (softmax and SVM) the two different weights initialization and the two learning rates used. The results of the table are the same when the learning rate is 0.01 independently of the weight initialization or the kernel used to classify. The metrics rate are not too bad, with the learning rate the results are worse, the learning rate is too big.\\

\begin{table}[htb]
\centering
\begin{tabular}{|ccccccc|}
\hline
Classifier &  Weight initialization & learning rate & TP  & TN  & FP  & FN \\ \hline
Softmax    &         Normal        &     0.001     & 61  & 609 &  3  & 7  \\
Softmax    &       Gaussian        &     0.001     & 66  & 605 &  7  & 2   \\
SVM RBF(C=5)&         Normal       &     0.01      & 63  & 593 & 19  & 5  \\
SVM RBF(C=5)&         Gaussian     &     0.01      &  63 & 593 & 19  &5  \\
SVM lineal(C=5)&      Gaussian     &     0.01      &  63 & 593 & 19  &5  \\
\hline
\end{tabular} \label{fravv}

\end{table}


\subsubsection{CASIA results}
For Casia, different experiments has been carried out in order to train the network, using SOFTMAX as classifier and normal weight initialization:\\

\begin{itemize}
\item{Test 1}: Learning rate = 0.01 y 400 epoch.
\item{Test 2}: Learning rate = 0.001 y 400 epoch.
\item{Test 3}: Learning rate = 0.0005 y 400 epoch.
\item{Test 4}: Learning rate = 0.001 y 1000 epoch.
\end{itemize}


In figure \ref{fig:lossCasia4Test} the cost at training could be visualized. In general, should decrease varying its value but decreasing logarithmically. In the first experiment (test 1), the value varies but in a small range, and in general it is constant, in the other three experiments, the cost decreases and this is what must happen, but in test 2 the loss converges two times, after converge the first time, then it increase again ans converges again.\\

In figure \ref{fig:errorCasia4Test} it is possible to see that the error changes but not in a logarithmically way. It increase and decreases its value in each epoch but it all experiments it gets in a 42\% or 40\% error. The desired curve should be as the loss one, but the error should not decrease as much as the training loss does.\\


\begin{figure}[htb]
\centering
		\subfigure[Test 1]{\includegraphics[width=0.42\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/prueba1/cost_frav_p1.png}}
		\subfigure[Test 2]{\includegraphics[width=0.42\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/prueba2/cost_frav_p2.png}}			\subfigure[Test 3]{\includegraphics[width=0.47\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/prueba3/cost_frav_p3.png}}
		\subfigure[Test 4]{\includegraphics[width=0.42\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/prueba4/cost_frav_p4.png}}
\caption{Training loss of four test Casia database}
\label{fig:lossCasia4Test}
\end{figure}

\begin{figure}[htb]
\centering
		\subfigure[Test 1]{\includegraphics[width=0.42\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/prueba1/error_frav_p1.png}}
		\subfigure[Test 2]{\includegraphics[width=0.42\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/prueba2/error_frav_p2.png}}
		\subfigure[Test 3]{\includegraphics[width=0.42\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/prueba3/error_frav_p3.png}}
		\subfigure[Test 4]{\includegraphics[width=0.42\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/prueba4/error_frav_p4.png}}
\caption{Validation error of four test Casia database}
\label{fig:errorCasia4Test}
\end{figure}


As the same way that in the first experiment that FRAV database converges is that would be expected from Casia, but it has not gotten.\\

\clearpage
At the same way as FRAV, the classifier and the weight initialization has been changed in order to know how the networks behavior with these changes. The learning rate used for this experiment is 0.01. First, the cost (at training) and the error (at validating) are going to be visualized when the weight initialization is normal \ref{fig:Imagenet2-casia-svm_general}. Second when the weight initialization used is Gaussian \ref{fig:Imagenet2-casia-svm_gauss}.\\

%\begin{figure}[htb]
%\centering
%\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/%casia_gaussian_init/cost_frav.png}
%\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/frav/%casia_gaussian_init/error_frav.png}
%\caption{Cost at training and error at testing - casia\_gaussian\_init} %\label{fig:Imagenet2-casia_gaussian_init}
%\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/svm_gauss/cost.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/svm_gauss/error.png}
\caption{Cost at training and error at validating -casia svm\_gauss.} \label{fig:Imagenet2-casia-svm_gauss}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/svm_general/cost.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/casia/svm_general/error.png}
\caption{Cost at training and error at validating -casia svm\_general.} \label{fig:Imagenet2-casia-svm_general}
\end{figure}

As the same way than in FRAV database, the cost and at training the error at validating has not changed. The positive and negative rates are the following ones in the three experiments:  TP = 8; TN = 21; FP = 3; FN = 8.\\


\clearpage
\subsubsection{MFSD results}

%\begin{figure}[htb]
%\centering
%\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/mfsd/mfsd_gaussian_init/cost_frav.png}
%\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/mfsd/mfsd_gaussian_init/error_frav.png}
%\caption{Cost at training and error at testing - mfsd\_gaussian\_init} %\label{fig:Imagenet2-mfsd_gaussian_init}
%\end{figure}

As the same way in FRAV and CASIA, but now with MFSD, it is going to be tested the network with a learning rate of 0.01, Gaussian initialization \ref{fig:Imagenet2-mfsd-svm_gauss} and normal distribution initialization \ref{fig:Imagenet2-mfsd-svm_general} and classifying the test with SVM (RBF kernel and linear) and softmax.\\

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/mfsd/svm_gauss/cost.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/mfsd/svm_gauss/error.png}
\caption{Cost at training and error at validating - mfsd svm\_gauss.} \label{fig:Imagenet2-mfsd-svm_gauss}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/mfsd/svm_general/cost.png}
\includegraphics[width=0.45\textwidth]{images/FRAv_casia_ImageNet/Imagenet2/mfsd/svm_general/error.png}
\caption{Cost at training and error at validating - mfsd svm\_general.} \label{fig:Imagenet2-mfsd-svm_general}
\end{figure}

The same problem occurs. The train and valid graphs are the same in both cases, independently of the initialization.\\ Also, the result at testing is the same in three cases: TP = 12, TN = 3, FP = 105, FN = 0\\

\clearpage
\subsection{Architecture}
%Ejecucuion1
The new architecture would be formed by the following layers:
\begin{itemize}[noitemsep,topsep=8pt,parsep=0pt,partopsep=20pt]
\item Convolutional layer with 96 kernels of size 11x11 followed by a max-pool layer (2x2 size) and a local response normalization.
\item Convolutional layer with 256 kernels whose size is 4x4.
\item Convolutional layer with 386 kernels whose size is 3x3.
\item Convolutional layer with 384 kernels whose size is 3x3.
\item Convolutional layer with 256 kernels whose size is 3x3 followed by a max-pool layer of 2x2 size.
\item Dropout layer with 4096 neurons.
\item Dropout layer with 4096 neurons.
\item Fully Connected layer with 2000 neurons at the output.
\end{itemize}

ReLu has been used as activation function. Logistic regression has been used in the training process. Weights have been initialized with Gaussian initialization and bias to 1.\\

Some experiments are going to be developed, and parameters like the learning rate would be changed to obtain the optimal one and those would be explained for each one.\\

\subsection{Databases}
The databases used in for experiments are CASIA images, CASIA videos, RGB FRAV, (RGB+NIR) FRAV feature level database and MFSD-MSU database. Just two classes are going to be used in this experiment: class 0 is real users class and class 1 is attack class.\\

Samples distribution are described in table \ref{Samples_distribution} where the number of samples per class and  subset (train, test and validation) are summarized for each database. It is possible to realize that the number of positive (class 0) samples is lower for all the databases, but more specifically, the amount of positive samples in MFSD-MSU database is very small.\\

\begin{table}[]
\centering
\begin{tabular}{c|c|c|c|c|c|}
\cline{2-6}
                       & \cellcolor[HTML]{ECF4FF}\begin{tabular}[c]{@{}l@{}}RGB\\ FRAV\end{tabular} & \cellcolor[HTML]{ECF4FF}\begin{tabular}[c]{@{}l@{}}RGB+NIR\\ FRAV\end{tabular} & \cellcolor[HTML]{ECF4FF}\begin{tabular}[c]{@{}l@{}}Images\\ CASIA\end{tabular} & \cellcolor[HTML]{ECF4FF}\begin{tabular}[c]{@{}l@{}}Videos\\ CASIA\end{tabular} & \cellcolor[HTML]{ECF4FF}MFSD-MSU \\ \hline

\multicolumn{1}{|c|}{\cellcolor[HTML]{ECF4FF}Train samples class 0} & 157                              & 133                                    & 26                                   & 255                                  & 30                           \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{ECF4FF}Train samples class 1} & 459                              & 417                                    & 111                                  & 81                                   & 68                           \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{ECF4FF}Valid samples class 0} & 10                               & 8                                      & 7                                    & 105                                  & 2                            \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{ECF4FF}Valid samples class 1} & 83                               & 70                                     & 13                                   & 39                                   & 12                           \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{ECF4FF}Test samples class 0}  & 19                               & 16                                     & 16                                   & 540                                  & 3                            \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{ECF4FF}Test samples class 1}  & 167                              & 70                                     & 23                                   & 180                                  & 25                           \\ \hline
\end{tabular}\caption{Samples distribution for each database}
\label{Samples_distribution}
\end{table}

\subsection{Experiments Description}
With that databases. Some experiments has been carried out:

\begin{itemize}
\item general experiment, with Gaussian weight initialization, classification with SVM RBF and SOFTMAX.
\item general experiment but with a small database in order to make over-fitting in the network and check it. It has been used 20 train, test and validation images in all databases but MFSD that has been used 14 images for each subset.
\item The same experiment that above but the test has been realized with the same subset that in training, this is to check that the network has over-fit or should have over-fitted.
\item The same as above but decreasing the learning rate from 0.01 to 0.001. \\
\end{itemize}


\begin{figure}[htb]
\centering
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_frav/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_frav/error.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_frav/minidataset/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_frav/minidataset/error.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_frav/minidataset_tested_itself/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_frav/minidataset_tested_itself/error.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_frav/minidataset_tested_iteself_lr_0_001/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_frav/minidataset_tested_iteself_lr_0_001/error.png}
\caption{cost and error of the tree experiments with frav.} \label{fig:frav-ejec1}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_frav_rgb_nir/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_frav_rgb_nir/error.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_frav_rgb_nir/minidataset/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_frav_rgb_nir/minidataset/error.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_frav_rgb_nir/minidataset_tested_itself/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_frav_rgb_nir/minidataset_tested_itself/error.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_frav_rgb_nir/minidataset_tested_iteself_lr_0_001/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_frav_rgb_nir/minidataset_tested_iteself_lr_0_001/error.png}
\caption{cost and error of the tree experiments with FRAV (rgb + nir) image level images.} \label{fig:frav_imagelevel-ejec1}
\end{figure}


\begin{figure}[htb]
\centering
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_casia/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_casia/error.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_casia/minidataset/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_casia/minidataset/error.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_casia/minidataset_tested_itself/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_casia/minidataset_tested_itself/error.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_casia/minidataset_tested_iteself_lr_0_001/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_casia/minidataset_tested_iteself_lr_0_001/error.png}
\caption{cost and error of the tree experiments with CASIA images.} \label{fig:casia-ejec1}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_casia_video/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_casia_video/error.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_casia_video/minidataset/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_casia_video/minidataset/error.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_casia_video/minidataset_tested_itself/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_casia_video/minidataset_tested_itself/error.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_casia_video/minidataset_tested_iteself_lr_0_001/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_casia_video/minidataset_tested_iteself_lr_0_001/error.png}
\caption{cost and error of the tree experiments with CASIA videos.} \label{fig:casiavid-ejec1}
\end{figure}

\begin{figure}[htb]
\centering
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_mfsd/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_mfsd/error.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_mfsd/minidataset/minidatasetcost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_mfsd/minidataset/minidataseterror.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_mfsd/minidataset_tested_itself/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_mfsd/minidataset_tested_itself/error.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_mfsd/minidataset_tested_iteself_lr_0_001/cost.png}
\includegraphics[width=0.42\textwidth]{images/redes/ejecucion1/general_svm_mfsd/minidataset_tested_iteself_lr_0_001/error.png}
\caption{cost and error of the tree experiments with MFSD images.} \label{fig:mfsd-ejec1}
\end{figure}

From images, could be concluded that in general, decreasing the learning rate for this experiment has not been a good idea.\\

In table \ref{table-ej1} could be seen the positive and negatives rates when has been used SVM with RBF kernel to classify. The result obtained with FRAV (rgb + NIR) is really good because just 3 samples have been misclassified from 140 images.\\

I do not know why the test is the same for minidataset and minidataset tested with itself.\\

\begin{table}[htb]
\centering
\label{table-ej1}
\begin{tabular}{cccccc}
-              &Optima C SVM& TP & TN & FP & FN \\
FRAV           &    0.05   & 136& 24 &  11 & 9 \\
FRAV (rgb+nir) &    0.1    & 113& 24 &  1  & 2 \\
CASIA images   &    5      & 9  & 2  &  0  & 9 \\
CASIA videos   &    0.1    & 478& 75 &  105& 62 \\
MFSD           &    10     & 19 &  1 &   8 & 0 \\
\end{tabular}
\end{table}

Looking the graphs where a minidataset has been used (20 images or 14), if the cost (training) is visualized, could be expected that the train is learning the images because the cost decreases to 0 (almost zero) so that means that if it is tested with itself the error should be 0, but that does not happen.\\

The conclusion is the needed of a balanced database, at least to do this experiment. In which the number of class 0 samples are the same that the number of class 1 samples, because the network would not learn in the same way if in some cases the number of samples of  attack class is four times than the number of samples of class 1, just predicting 0 would have 25\% accuracy, and having less than 5 samples in validation or testing is not a good generalizer (2 samples in class 0 MFSD database).\\

\section{Final architecture}\label{Final_archi} %Used in ejecucion 2
The architecture utilized in the final, and the most important experiment is described in this section.\\

The neural network is composed by five convolutional layers: the first and second convolutional layers (CL1 and CL2) , whose kernel sizes are 11x11 and 3x3 respectively, are followed by a local response normalization layer and a max pool layer whose size is 2x2. The third convolutional layer (CL3), with a kernel size of 3x3 , the next layer is a convolutional one (CL4) of size 3x3 followed by a max-pool layer whose size is 2x2. The next two layers are Dropouts layers (DL1 and DL2) with 4096 neurons. The next layer us a Fully-connected layer (FL) with 4096 neurons at the input and 2000 layers at the output.\\

The activation function used in each layer is the ReLu. The weights have been initialized pseudo-randomly (a random initialization that could be repeated selecting he same seed) with a Gaussian distribution and the bias has been initialized with 1.\\

It has been used the minibatch Stochastic Gradient Descend and in the training, the used classifier, is the logistic regression.\\

The learning rate is fixed at a 0.01 value and a bath size of 20, except when the MFSD database is used that he batch size is 14.\\

The dataset used in this experiment are the FRAV with the only RGB images, the FRAV database with the RGB and NIR added at the characteristic level  and  classifier level. The MFSD database has been used too and both CASIA database have been used, image and video CASIA database. For each database, it has been split randomly into train, test and validate subsets. Two classes has been used, class 0: the real users class and class 1: the attacks class. In next table \ref{Samples_distribution} is possible to visualize the samples distribution for each subset (train, test, validation). There is just one row for RGB+NIR FRAV database because the sample distribution is the same independently of when the are concatenate.\\




For testing some classifiers has been utilized, and they are fed by the output of the convolutional neural network last layer, the Fully-connected layer.\\

The classifiers used to classify the features of the output of the CNN and get the results are the SVM (with RBF and linear kernel), KNN, Decision Tree and logistic regression. Also, PCA and LDA techniques has been used with each classifier separately to reduce the dimensionality of the features.\\

The classifiers, before use them, has been personalize to each and particular time (for each database and if LDA or PCA is used). For that, cross validation has been used, more concretely, the \textit{cros\_val\_Score()} function from sklearn has used with 10 folders.\\

\begin{itemize}
\item For SMV classifier, the C parameter has been searched among the following values: 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2, 3, 5 and 10.
\item For KNN classifier, the number of neighbors (K) has been searched among the following values: 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28 and 30.
\item For Deep Trees classifier, the depth of the tree has been searches among the following values: 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28 and 30.
\item For PCA, the number of components has been found in a range of 3 to 5000, in 3 to 3 steps.
\item For LDA, the number of components has been found in a range of 1 to the length of the characteristic vector in 10 to 10 steps.
\end{itemize}

%With the purpose of having a more general results, all the training, validation and testing (with the classification) processes have been repeated three times, those times are differentiated among them in the seed if the pseudo-randomized initialization of the weights.\\
\clearpage
